{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responsible AI: XAI GenAI project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Background\n",
    "\n",
    "\n",
    "\n",
    "Based on the previous lessons on explainability, post-hoc methods are used to explain the model, such as saliency map, SmoothGrad, LRP, LIME, and SHAP. Take LRP (Layer Wise Relevance Propagation) as an example; it highlights the most relevant pixels to obtain a prediction of the class \"cat\" by backpropagating the relevance. (image source: [Montavon et. al (2016)](https://giorgiomorales.github.io/Layer-wise-Relevance-Propagation-in-Pytorch/))\n",
    "\n",
    "<!-- %%[markdown] -->\n",
    "![LRP example](images/catLRP.jpg)\n",
    "\n",
    "Another example is about text sentiment classification, here we show a case of visualizing the importance of words given the prediction of 'positive':\n",
    "\n",
    "![text example](images/textGradL2.png)\n",
    "\n",
    "where the words highlight with darker colours indicate to be more critical in predicting the sentence to be 'positive' in sentiment.\n",
    "More examples could be found [here](http://34.160.227.66/?models=sst2-tiny&dataset=sst_dev&hidden_modules=Explanations_Attention&layout=default).\n",
    "\n",
    "Both cases above require the class or the prediction of the model. But:\n",
    "\n",
    "***How do you explain a model that does not predict but generates?***\n",
    "\n",
    "In this project, we will work on explaining the generative model based on the dependency between words. We will first look at a simple example, and using Point-wise Mutual Information (PMI) to compute the saliency map of the sentence. After that we will contruct the expereiment step by step, followed by exercises and questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A simple example to start with\n",
    "Given a sample sentence: \n",
    "> *Tokyo is the capital city of Japan.* \n",
    "\n",
    "We are going to explain this sentence by finding the dependency using a saliency map between words.\n",
    "The dependency of two words in the sentence could be measured by [Point-wise mutual information (PMI)](https://en.wikipedia.org/wiki/Pointwise_mutual_information): \n",
    "\n",
    "\n",
    "Mask two words out, e.g. \n",
    "> \\[MASK-1\\] is the captial city of \\[MASK-2\\].\n",
    "\n",
    "\n",
    "Ask the generative model to fill in the sentence 10 times, and we have:\n",
    "\n",
    "| MASK-1      | MASK-2 |\n",
    "| ----------- | ----------- |\n",
    "|    tokyo   |     japan   |\n",
    "|  paris  |     france    |\n",
    "|  london  |     england    |\n",
    "|  paris  |     france    |\n",
    "|  beijing |  china |\n",
    "|    tokyo   |     japan   |\n",
    "|  paris  |     france    |\n",
    "|  paris  |     france    |\n",
    "|  london  |     england    |\n",
    "|  beijing |  china |\n",
    "\n",
    "PMI is calculated by: \n",
    "\n",
    "$PMI(x,y)=log_2⁡ \\frac{p(\\{x,y\\}| s-\\{x,y\\})}{P(\\{x\\}|s-\\{x,y\\})P(\\{y\\}|s-\\{x,y\\})}$\n",
    "\n",
    "where $x$, $y$ represents the words that we masked out, $s$ represents the setence, and $s-\\{x,y\\}$ represents the sentences tokens after removing the words $x$ and $y$.\n",
    "\n",
    "In this example we have $PMI(Tokyo, capital) = log_2 \\frac{0.2}{0.2 * 0.2} = 2.32$\n",
    "\n",
    "Select an interesting word in the sentences; we can now compute the PMI between all other words and the chosen word using the generative model:\n",
    "(Here, we use a longer sentence and run 20 responses per word.)\n",
    "![](images/resPMI.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparation\n",
    "### 2.1 Conda enviroment\n",
    "\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate xai_llm\n",
    "```\n",
    "\n",
    "\n",
    "### 2.2 Download the offline LLM\n",
    "\n",
    "We use the offline LLM model from hugging face. It's approximately 5 GB.\n",
    "Download it using the comman below, and save it under `./models/`.\n",
    "```\n",
    "huggingface-cli download TheBloke/openchat-3.5-0106-GGUF openchat-3.5-0106.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n",
    "# credit to https://huggingface.co/TheBloke/openchat-3.5-0106-GGUF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mask the sentence and get the responses from LLM\n",
    "### 3.1 Get the input sentence\n",
    "\n",
    "**Remember to change the anchor word index when changing the input sentence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Tokyo is the capital city of Japan.\n"
     ]
    }
   ],
   "source": [
    "def get_input():\n",
    "    # ideally this reads inputs from a file, now it just takes an input\n",
    "    return input(\"Enter a sentence: \")\n",
    "    \n",
    "anchor_word_idx = 0 # the index of the interested word\n",
    "prompts_per_word = 20 # number of generated responses  \n",
    "\n",
    "#sentence = get_input()\n",
    "sentence = \"Tokyo is the capital city of Japan.\"\n",
    "print(\"Sentence: \", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: openchat\n"
     ]
    }
   ],
   "source": [
    "from models.ChatModel import ChatModel\n",
    "model_name = \"openchat\"\n",
    "model = ChatModel(model_name)\n",
    "print(f\"Model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run the prompts and get all the responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] the capital city of Japan.:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] the capital city of Japan.: 100%|██████████| 20/20 [00:29<00:00,  1.47s/it]\n",
      "Input: [MASK] is [MASK] capital city of Japan.:  40%|████      | 8/20 [00:12<00:17,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', '[mask]', 'capital', 'city', 'of', 'japan'] ['tokyo', 'is', 'japans', 'capital', 'city']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is [MASK] capital city of Japan.:  85%|████████▌ | 17/20 [00:25<00:04,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', '[mask]', 'capital', 'city', 'of', 'japan'] ['tokyo', 'is', 'japans', 'capital', 'city']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is [MASK] capital city of Japan.: 100%|██████████| 20/20 [00:29<00:00,  1.47s/it]\n",
      "Input: [MASK] is the [MASK] city of Japan.: 100%|██████████| 20/20 [00:33<00:00,  1.66s/it]\n",
      "Input: [MASK] is the capital [MASK] of Japan.:  45%|████▌     | 9/20 [00:12<00:14,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', 'the', 'capital', '[mask]', 'of', 'japan'] ['tokyo', 'is', 'the', 'capital', '[japan]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is the capital [MASK] of Japan.: 100%|██████████| 20/20 [00:26<00:00,  1.30s/it]\n",
      "Input: [MASK] is the capital city [MASK] Japan.: 100%|██████████| 20/20 [00:27<00:00,  1.39s/it]\n",
      "Input: [MASK] is the capital city of [MASK]: 100%|██████████| 20/20 [00:28<00:00,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "from tools.command_generator import generate_prompts, prefix_prompt\n",
    "from tools.evaluate_response import get_replacements\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_prompts(model, sentence, anchor_idx, prompts_per_word=20,blob =False):\n",
    "    prompts = generate_prompts(sentence, anchor_idx, blob)\n",
    "    all_replacements = []\n",
    "    for prompt in prompts:\n",
    "        replacements = []\n",
    "        for _ in tqdm(\n",
    "            range(prompts_per_word),\n",
    "            desc=f\"Input: {prompt}\",\n",
    "        ):\n",
    "            response = model.get_response(\n",
    "                prefix_prompt(prompt),\n",
    "            ).strip()\n",
    "            if response:\n",
    "                replacement = get_replacements(prompt, response)\n",
    "                if replacement:\n",
    "                    replacements.append(replacement)\n",
    "        if len(replacements) > 0:\n",
    "            all_replacements.append(replacements)\n",
    "    return all_replacements\n",
    "\n",
    "all_responses = run_prompts(model, sentence, anchor_word_idx, prompts_per_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 EXERCISE: compute the PMI for each word\n",
    "\n",
    "$PMI(x,y)=log_2⁡ \\frac{p(\\{x,y\\}| s-\\{x,y\\})}{P(\\{x\\}|s-\\{x,y\\})P(\\{y\\}|s-\\{x,y\\})}$\n",
    "\n",
    "* Compute the $P(x)$, $P(y)$ and $P(x,y)$ first and print it out.\n",
    "* Compute the PMI for each word.\n",
    "* Visualize the result by coloring. Tips: you might need to normalize the result first. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is']]\n",
      "[['tokyo', 'tokyos'], ['tokyo', 'tokyos'], ['tokyo', 'tokyos'], ['tokyo', 'the'], ['tokyo', 'the'], ['tokyo', '[mask]'], ['tokyo', 'the'], ['', ''], ['tokyo', 'tokyos'], ['tokyo', 'the'], ['tokyo', 'tokyos'], ['tokyo', 'the'], ['tokyo', 'the'], ['tokyo', 'the'], ['tokyo', 'the'], ['tokyo', 'the'], ['', ''], ['tokyo', 'the'], ['tokyo', 'the'], ['tokyo', 'the']]\n",
      "[['osaka', 'third largest'], ['osaka', 'second largest'], ['tokyo', 'capital'], ['osaka', 'second largest'], ['tokyo', 'capital'], ['kyoto', 'former [mask]'], ['osaka', 'secondlargest'], ['hiroshima', 'third largest'], ['osaka', 'second largest'], ['osaka', 'second largest'], ['osaka', 'second largest'], ['osaka', 'second largest'], ['hiroshima', 'second largest'], ['tokyo', 'largest'], ['tokyo', 'capital'], ['tokyo', 'capital'], ['kyoto', 'ancient'], ['osaka', 'second largest'], ['osaka', 'second largest'], ['nagasaki', 'second largest']]\n",
      "[['tokyo', 'city'], ['tokyo', 'city'], ['tokyo', 'city'], ['tokyo', 'city'], ['tokyo', ''], ['tokyo', ''], ['tokyo', ''], ['tokyo', ''], ['', ''], ['tokyo', ''], ['tokyo', ''], ['tokyo', 'city'], ['tokyo', 'city'], ['tokyo', ''], ['tokyo', 'tokyo'], ['tokyo', ''], ['tokyo', 'city'], ['tokyo', ''], ['tokyo', ''], ['tokyo', 'city']]\n",
      "[['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of']]\n",
      "[['london', 'united kingdom'], ['tokyo', 'japan'], ['tokyo', 'japan'], ['tokyo', 'japan'], ['tokyo', 'japan'], ['paris', 'france'], ['tokyo', 'japan'], ['london', 'the united kingdom'], ['paris', 'france'], ['tokyo', 'japan'], ['paris', 'france'], ['new delhi', 'india'], ['paris', 'france'], ['london', 'united kingdom'], ['paris', 'france'], ['london', 'england'], ['tokyo', 'japan'], ['tokyo', 'japan'], ['london', 'the united kingdom'], ['tokyo', 'japan']]\n"
     ]
    }
   ],
   "source": [
    "#print (all_responses)\n",
    "for reponse in all_responses:\n",
    "    print(reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/s184361/XAI_GenAI_project_release/sentences.py:82: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.000000000005' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  p_df.at['px', word_y] = px\n",
      "/work3/s184361/XAI_GenAI_project_release/sentences.py:82: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.9000000000050001' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  p_df.at['px', word_y] = px\n",
      "/work3/s184361/XAI_GenAI_project_release/sentences.py:82: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.250000000005' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  p_df.at['px', word_y] = px\n",
      "/work3/s184361/XAI_GenAI_project_release/sentences.py:82: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.9500000000050001' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  p_df.at['px', word_y] = px\n",
      "/work3/s184361/XAI_GenAI_project_release/sentences.py:82: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.000000000005' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  p_df.at['px', word_y] = px\n",
      "/work3/s184361/XAI_GenAI_project_release/sentences.py:82: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.450000000005' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  p_df.at['px', word_y] = px\n"
     ]
    }
   ],
   "source": [
    "from sentences import calculate_pmis\n",
    "\n",
    "p_df = calculate_pmis(sentence, all_responses, anchor_word_idx, prompts_per_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          tokyo            is       the  capital      city            of  \\\n",
      "px          NaN  1.000000e+00  0.900000     0.25  0.950000  1.000000e+00   \n",
      "py          NaN  1.000000e+00  0.600000     0.20  0.400000  1.000000e+00   \n",
      "pxy         NaN  1.000000e+00  0.600000     0.20  0.400000  1.000000e+00   \n",
      "pmi         NaN -7.213476e-12  0.152003     2.00  0.074001 -7.213476e-12   \n",
      "saliency    NaN -3.606738e-12  0.076002     1.00  0.037000 -3.606738e-12   \n",
      "\n",
      "             japan  \n",
      "px        0.450000  \n",
      "py        0.450000  \n",
      "pxy       0.450000  \n",
      "pmi       1.152003  \n",
      "saliency  0.576002  \n",
      "\u001b[32mTokyo\u001b[0m \u001b[32mis\u001b[0m \u001b[32mthe\u001b[0m \u001b[31mcapital\u001b[0m \u001b[32mcity\u001b[0m \u001b[32mof\u001b[0m \u001b[31mJapan.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(p_df)\n",
    "#print the sentence colored with the saliency values\n",
    "from sentences import colorize_sentence\n",
    "\n",
    "saliency = p_df.loc['saliency'].values\n",
    "colored_sentence = colorize_sentence(sentence, saliency)\n",
    "print(colored_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. EXERCISE: Try more examples; maybe come up with your own. Report the results.\n",
    "\n",
    "* Try to come up with more examples and, change the anchor word/number of responses, and observe the results. What does the explanation mean? Do you think it's a nice explanation? Why and why not? \n",
    "* What's the limitation of the current method? When does the method fail to explain? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bonus Exercises\n",
    "### 5.1 Language pre-processing. \n",
    "In this exercise, we only lower the letters and split sentences into words; there's much more to do to pre-process the language. For example, contractions (*I'll*, *She's*, *world's*), suffix and prefix, compound words (*hard-working*). It's called word tokenization in NLP, and there are some Python packages that can do such work for us, e.g. [*TextBlob*](https://textblob.readthedocs.io/en/dev/). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Japan', \"'s\", 'capital', 'is', 'Tokyo']\n",
      "['Japanese', 'people', 'are', 'hard-working']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "sentence = \"Japan's capital is Tokyo.\"\n",
    "print(TextBlob(sentence).words)\n",
    "\n",
    "prompts = generate_prompts(sentence, 4, True)\n",
    "\n",
    "sentence = \"Japanese people are hard-working.\"\n",
    "print(TextBlob(sentence).words)\n",
    "\n",
    "all_responses = run_prompts(model, sentence, anchor_word_idx, prompts_per_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Better word matching\n",
    "In the above example of\n",
    "> Tokyo is the capital of Japan and a popular metropolis in the world.\n",
    "\n",
    ", GenAI never gives the specific word 'metropolis' when masking it out; instead, sometimes it provides words like 'city', which is not the same word but has a similar meaning. Instead of measuring the exact matching of certain words (i.e. 0 or 1), we can also measure the similarity of two words, e.g. the cosine similarity in word embedding, which ranges from 0 to 1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
